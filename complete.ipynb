{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b06ebe2",
   "metadata": {},
   "source": [
    "1. Load and inspect the data, plot few examples from different arrythmia types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66994cd0-41da-4bb5-a545-2b33969e3d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and inspect the data, plot few examples from different arrythmia types\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbeecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1.Loading and inspecting heartbeat holdout validation data\n",
    "print(\"1.  Loading and inspecting heartbeat holdout validation data\")\n",
    "train_files = glob.glob('train_beats.csv') \n",
    "test_files = glob.glob('test_beats.csv')   \n",
    "if not train_files or not test_files:\n",
    "    raise FileNotFoundError(\"Heartbeat training or test data files not found. Please ensure data_split_resample.ipynb has been run correctly and generated the files.\")\n",
    "\n",
    "# load data\n",
    "train_data = pd.read_csv(train_files[0], header=None)\n",
    "test_data = pd.read_csv(test_files[0], header=None)\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nTraining set class distribution:\")\n",
    "# print(train_data.iloc[:, -1].value_counts().sort_index())\n",
    "print(\"\\nTest set class distribution:\")\n",
    "# print(test_data.iloc[:, -1].value_counts().sort_index())\n",
    "\n",
    "# Prepare data\n",
    "X_train = train_data.iloc[:, :-2].values \n",
    "y_train = train_data.iloc[:, -2].values  \n",
    "X_test = test_data.iloc[:, :-2].values\n",
    "y_test = test_data.iloc[:, -2].values\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "\n",
    "# Data standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nData loading and preprocessing completed.\")\n",
    "\n",
    "print(\"\\n2. Plotting examples of different arrhythmia types...\")\n",
    "class_names = {\n",
    "    1: 'N (Normal)',\n",
    "    2: 'L (LBBB)',\n",
    "    3: 'R (RBBB)',\n",
    "    4: 'V (Premature Ventricular Contraction)',\n",
    "    5: 'A (Atrial premature beat)',\n",
    "    6: 'F (Fusion ventricular normal beat)',\n",
    "    7: 'f (Fusion of paced and normal beat)',\n",
    "    8: '/ (Paced beat)'\n",
    "}\n",
    "fig, axes = plt.subplots(4, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "for class_id in range(1, 9):\n",
    "    if class_id in y_train:\n",
    "        idx = np.where(y_train == class_id)[0][0]  \n",
    "        axes[class_id-1].plot(X_train[idx])\n",
    "        axes[class_id-1].set_title(class_names[class_id])\n",
    "        axes[class_id-1].set_xlabel(\"Time\")\n",
    "        axes[class_id-1].set_ylabel(\"Amplitude\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f35a5c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2b79a67",
   "metadata": {},
   "source": [
    "2. Classification of ECG beats based on the holdout splitting method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd937cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  2.Heartbeat Holdout Validation Classification Model\n",
    "\n",
    "# create svm model\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "y_pred = svm_model.predict(X_test_scaled)\n",
    "# Evaluation\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    # Visualize confusion matrix cm\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
    "    plt.title('Confusion Matrix (Heartbeat Holdout Validation)')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "    return accuracy, precision, recall, f1, cm\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nHeartbeat Holdout Validation SVM Model Evaluation Results\")\n",
    "acc, prec, rec, f1, cm = evaluate_model(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f96d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model Interpretability Analysis\n",
    "print(\"\\n3. Using permutation feature importance to interpret model functionality\")\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.inspection import permutation_importance  \n",
    "print(\"\\nSVM Classifier\")\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "print(\"\\nHeartbeat Holdout Validation SVM Model Evaluation Results\")\n",
    "acc_svm, prec_svm, rec_svm, f1_svm, cm_svm = evaluate_model(y_test, y_pred_svm)\n",
    "print(\"\\nRandom Forest Classifier\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "acc_rf, prec_rf, rec_rf, f1_rf, cm_rf = evaluate_model(y_test, y_pred_rf)\n",
    "# 4. Apply the same permutation feature importance analysis to different classifiers\n",
    "print(\"\\n4.Apply the same permutation feature importance analysis to different classifiers\")\n",
    "\n",
    "def calculate_permutation_importance_cv(model, X, y, cv_folds=5, n_repeats=10, random_state=42, model_type=\"SVM\"):\n",
    "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
    "    importances = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"   Processing fold {fold+1}/{cv_folds} \")\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "        if model_type == \"SVM\":\n",
    "            fold_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "            fold_model.fit(X_train_fold, y_train_fold)\n",
    "            perm_imp = permutation_importance(\n",
    "                fold_model, X_val_fold, y_val_fold,\n",
    "                n_repeats=n_repeats,\n",
    "                random_state=random_state,\n",
    "                scoring='accuracy'\n",
    "            )\n",
    "        else:  # RandomForest\n",
    "            fold_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "            fold_model.fit(X_train_fold, y_train_fold)\n",
    "            perm_imp = permutation_importance(\n",
    "                fold_model, X_val_fold, y_val_fold,\n",
    "                n_repeats=n_repeats,\n",
    "                random_state=random_state,\n",
    "                scoring='accuracy'\n",
    "            )\n",
    "            \n",
    "        importances.append(perm_imp.importances_mean)\n",
    "    importances = np.array(importances)\n",
    "    mean_importance = np.mean(importances, axis=0)\n",
    "    std_importance = np.std(importances, axis=0)\n",
    "    return mean_importance, std_importance\n",
    "\n",
    "# visualize\n",
    "def plot_feature_importance(mean_importance, std_importance, title, top_n=50):\n",
    "    sorted_idx = np.argsort(mean_importance)[::-1]\n",
    "    top_indices = sorted_idx[:top_n]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(range(top_n), mean_importance[top_indices],\n",
    "            yerr=std_importance[top_indices], align='center')\n",
    "    plt.xlabel('Feature Index (Sorted by Importance)')\n",
    "    plt.ylabel('Permutation Importance (Drop in Accuracy)')\n",
    "    plt.title(f'Top {top_n} Feature Importances ({title})')\n",
    "    plt.xticks(range(top_n), [f'F{i}' for i in top_indices], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nTop 10 most important features (based on mean importance):\")\n",
    "    for i in range(min(10, top_n)):\n",
    "        idx = top_indices[i]\n",
    "        print(f\" Feature {idx}: Mean importance = {mean_importance[idx]:.5f},  Std= {std_importance[idx]:.5f}\")\n",
    "        \n",
    "print(\"\\nSVM Classifier Feature Importance Analysis\")\n",
    "n_samples_for_perm_imp = min(5000, X_train_scaled.shape[0])\n",
    "indices = np.random.choice(X_train_scaled.shape[0], n_samples_for_perm_imp, replace=False)\n",
    "X_train_subset_svm = X_train_scaled[indices]\n",
    "y_train_subset_svm = y_train[indices]\n",
    "\n",
    "mean_imp_svm, std_imp_svm = calculate_permutation_importance_cv(\n",
    "    svm_model, X_train_subset_svm, y_train_subset_svm, cv_folds=5, model_type=\"SVM\"\n",
    ")\n",
    "\n",
    "plot_feature_importance(mean_imp_svm, std_imp_svm, \"Heartbeat Holdout SVM\", top_n=50)\n",
    "\n",
    "print(\"\\nRandom Forest Classifier Feature Importance Analysis\")\n",
    "X_train_subset_rf = X_train_scaled[indices]\n",
    "y_train_subset_rf = y_train[indices]\n",
    "\n",
    "mean_imp_rf, std_imp_rf = calculate_permutation_importance_cv(\n",
    "    rf_model, X_train_subset_rf, y_train_subset_rf, cv_folds=5, model_type=\"RF\"\n",
    ")\n",
    "\n",
    "plot_feature_importance(mean_imp_rf, std_imp_rf, \"Heartbeat Holdout Random Forest\", top_n=50)\n",
    "\n",
    "print(\"\\nTask 5: Applying the same interpretability techniques to different types of classifiers completed\")\n",
    "print(\"- SVM and Random Forest classification models have been trained and evaluated\")\n",
    "print(\"- Permutation feature importance for both classifiers calculated using 5-fold stratified cross-validation.\")\n",
    "print(\"- Feature importance results have been visualized and compared.\")\n",
    "\n",
    "print(\"\\nClassifier Performance Comparison\")\n",
    "print(f\"SVM - Accuracy: {acc_svm:.4f}, F1-score: {f1_svm:.4f}\")\n",
    "print(f\"Random Forest - Accuracy: {acc_rf:.4f}, F1-score: {f1_rf:.4f}\")\n",
    "\n",
    "print(\"\\nFeature Importance Comparison\")\n",
    "top_features_svm = np.argsort(mean_imp_svm)[::-1][:20]\n",
    "top_features_rf = np.argsort(mean_imp_rf)[::-1][:20]\n",
    "\n",
    "print(\"SVM top 20 important feature indices:\", top_features_svm)\n",
    "print(\"Random Forest top 20 important feature indices:\", top_features_rf)\n",
    "\n",
    "overlap = len(set(top_features_svm) & set(top_features_rf))\n",
    "print(f\"Number of overlapping features in top 20 between both methods{overlap}/20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ca3574-061f-45cf-ae7d-2582dcffb24f",
   "metadata": {},
   "source": [
    " # Random Forest vs. SVM for ECG Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399480c6",
   "metadata": {},
   "source": [
    "## 1. Performance Overview\n",
    "Random Forest demonstrated superior performance over SVM across key evaluation metrics, with notable improvements in Accuracy, Recall, and F1-score, indicating its stronger capability in identifying abnormal heartbeats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dad45f-3699-4546-83a1-92d1e8b55fdd",
   "metadata": {},
   "source": [
    "| Metric                | SVM     | Random Forest |\n",
    "|-----------------------|---------|---------------|\n",
    "| **Accuracy**          | 0.9494  | 0.9667        |\n",
    "| **Precision (Weighted)** | 0.9722  | 0.9741        |\n",
    "| **Recall (Weighted)**    | 0.9494  | 0.9667        |\n",
    "| **F1-Score (Weighted)**  | 0.9577  | 0.9690        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a73854",
   "metadata": {},
   "source": [
    "## 2. Detailed Category-wise Misclassification Analysis\n",
    "Category 1 (Normal): In SVM, a large number were misclassified as Category 4 (Premature Ventricular Contraction) and Category 5 (Atrial Premature Beat), whereas Random Forest correctly classified almost all of them.\n",
    "\n",
    "Category 4 (Premature Ventricular Contraction): In SVM, approximately 323 instances were misclassified as Normal, while Random Forest misclassified only 19, showing significant improvement.\n",
    "\n",
    "Category 5 (Atrial Premature Beat): In SVM, 52 instances were misclassified as Normal, while in Random Forest, 56 were misclassified, though the overall trend is better.\n",
    "\n",
    "Category 6 (Fusion ventricular normal beat ): In SVM, 179 instances were misclassified as Normal, while in Random Forest, only 169 were misclassified, indicating a slight improvement.\n",
    "\n",
    "Conclusion: Random Forest demonstrates greater advantages in distinguishing between \"Normal\" and \"Abnormal\" heartbeats, particularly excelling in handling ambiguous premature beat samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290ba44",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c78d3057-4809-462b-bd16-1fed1d5b49bc",
   "metadata": {},
   "source": [
    "## 3. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc99b92",
   "metadata": {},
   "source": [
    "1.Top 50 Feature lmportances(Heartbeat Holdout SVM)（1lqpweqj.png）\n",
    "\n",
    "Top 10 Important Features: F133, F134, F138, F139, F89, F90, F137, F96, F155, F154  \n",
    "\n",
    "Distribution Characteristics:  \n",
    "(1) Feature importance is relatively uniform, with the maximum value being approximately 0.003.  \n",
    "(2) The standard deviation is large (e.g., F133: ±0.00152), indicating poor stability.  \n",
    "(3) The features are mainly concentrated in the F89-F155 interval, which may correspond to the QRS complex or T-wave region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e823423d",
   "metadata": {},
   "source": [
    "2. Top 50 Feature lmportances (Heartbeat Holdout Random Forest) (rtzp5c9p.png)  \n",
    "Top 10 Important Features: F145, F144, F155, F139, F117, F153, F128, F154, F90, F135  \n",
    "\n",
    "Distribution Characteristics:  \n",
    "(1) Feature importance is more concentrated, with F145 exceeding 0.006, significantly higher than other features.  \n",
    "(2) The distribution exhibits a \"long-tail\" pattern, indicating that a small number of features play a dominant role.  \n",
    "(3) Compared to SVM, higher-order features such as F145 and F144 are more critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e60a8f",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "The feature importance in SVM is more suitable for modeling linear relationships, making it effective for capturing localized signal variations.  \n",
    "In contrast, the feature importance in Random Forest reflects non-linear patterns and complex feature interactions, demonstrating its advantage in handling multi-dimensional morphological characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7cdbb0-bf4e-44c3-a44b-51af330b34a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec13a632",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c40fa5b",
   "metadata": {},
   "source": [
    "Random Forest demonstrates superior accuracy and robustness in ECG heartbeat classification tasks, exhibiting distinct advantages particularly in identifying complex arrhythmias. Although SVM's feature importance offers higher interpretability, Random Forest delivers better overall performance, making it more suitable as the primary classification model. By integrating the interpretability insights from both approaches, we can provide clinicians with accurate and trustworthy diagnostic support, ultimately enhancing the reliability and actionable insights of automated ECG analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b7011-be9d-488e-909a-5834e1cd3ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5edf2e5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "458bbd78",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbd897b0",
   "metadata": {},
   "source": [
    "3. Classification of ECG beats based on the leave out patients-hold out validation \n",
    "protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 8 target Class IDs\n",
    "CLASS_IDS = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# --- Required Evaluation Function ---\n",
    "\n",
    "def evaluate_model(Y_true, Y_pred, all_classes):\n",
    "    \"\"\"\n",
    "    Calculates and prints performance metrics (Accuracy, Precision, Recall, F1-score)\n",
    "    and the Confusion Matrix, handling cases where some classes might be missing from Y_true.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify only the unique labels present in the true test data (Y_true)\n",
    "    present_classes = np.unique(Y_true.astype(int))\n",
    "\n",
    "    # 1. Overall Accuracy\n",
    "    accuracy = accuracy_score(Y_true, Y_pred)\n",
    "\n",
    "    # 2. Overall Metrics (Micro average)\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "        Y_true, Y_pred, average='micro', zero_division=0, labels=present_classes)\n",
    "\n",
    "    # 3. Individual Class Metrics (Calculate only for present classes)\n",
    "    metrics_per_class = precision_recall_fscore_support(\n",
    "        Y_true, Y_pred, labels=present_classes, average=None, zero_division=0)\n",
    "\n",
    "    # 4. Confusion Matrix (Calculate partial matrix first)\n",
    "    # cm_partial = confusion_matrix(Y_true, Y_pred, labels=present_classes)\n",
    "    # The above would create a matrix only for present_classes.\n",
    "    # To ensure all relevant labels are included in the confusion matrix, we take a union.\n",
    "    all_relevant_labels = sorted(list(set(all_classes).union(set(Y_true.unique())).union(set(Y_pred))))\n",
    "    cm_final = confusion_matrix(Y_true, Y_pred, labels=all_relevant_labels)\n",
    "\n",
    "\n",
    "    # --- Reconstruct Output for all 8 Classes ---\n",
    "    class_names = ['N (1)', 'L (2)', 'R (3)', 'V (4)', 'A (5)', 'F (6)', 'f (7)', '/ (8)']\n",
    "    class_names_map = {class_id: class_name for class_id, class_name in zip(all_classes, class_names)}\n",
    "\n",
    "    metrics_map = {label: [p, r, f, s] for label, p, r, f, s in zip(present_classes, *metrics_per_class)}\n",
    "\n",
    "    # Create the final, complete 8-class metrics data structure (filling in 0s for missing classes)\n",
    "    final_metrics_data = []\n",
    "    for class_id, class_name in zip(all_classes, class_names):\n",
    "        if class_id in metrics_map:\n",
    "            p, r, f, s = metrics_map[class_id]\n",
    "        else:\n",
    "            # Class was missing from Y_true, fill with zeros\n",
    "            p, r, f, s = 0.0, 0.0, 0.0, 0\n",
    "\n",
    "        final_metrics_data.append({\n",
    "            'Class': class_name,\n",
    "            'Precision': p,\n",
    "            'Recall': r,\n",
    "            'F1-Score': f,\n",
    "            'Support': s\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(final_metrics_data)\n",
    "\n",
    "    # Reconstruct the Confusion Matrix for all 8 classes\n",
    "    # The original logic below caused the ValueError due to length mismatch.\n",
    "    # We now create the DataFrame based on all_relevant_labels and map names.\n",
    "\n",
    "    cm_full_df = pd.DataFrame(cm_final, index=all_relevant_labels, columns=all_relevant_labels)\n",
    "\n",
    "    # Map numerical labels to human-readable names where available, otherwise use generic names.\n",
    "    mapped_index = [class_names_map.get(label, f\"Class {label}\") for label in all_relevant_labels]\n",
    "    mapped_columns = [class_names_map.get(label, f\"Class {label}\") for label in all_relevant_labels]\n",
    "\n",
    "    cm_full_df.index = mapped_index\n",
    "    cm_full_df.columns = mapped_columns\n",
    "\n",
    "    print(f\"--- Task 2: SVC Model Evaluation (Beat Holdout Protocol) ---\")\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Overall Precision (Micro Avg): {precision_micro:.4f}\")\n",
    "    print(f\"Overall Recall (Micro Avg):    {recall_micro:.4f}\")\n",
    "    print(f\"Overall F1-Score (Micro Avg):  {f1_micro:.4f}\")\n",
    "    print(\"\\n--- Detailed Metrics per Class ---\")\n",
    "    print(metrics_df.to_string(index=False))\n",
    "\n",
    "    print(\"\\n--- Confusion Matrix (True Label Rows vs Predicted Label Columns) ---\")\n",
    "    if len(all_relevant_labels) > len(all_classes):\n",
    "        print(\"Note: Confusion matrix labels include additional classes found in the data, not just the predefined 8 CLASS_IDS.\")\n",
    "    print(cm_full_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be7935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading and Preparation ---\n",
    "\n",
    "TRAIN_FILE = './train_beats.csv'\n",
    "TEST_FILE = './test_beats.csv'\n",
    "\n",
    "print(f\"Loading data from {TRAIN_FILE} and {TEST_FILE}...\")\n",
    "\n",
    "try:\n",
    "    # Load the datasets (header=None as they are saved without headers)\n",
    "    train_data = pd.read_csv(TRAIN_FILE, header=None)\n",
    "    test_data = pd.read_csv(TEST_FILE, header=None)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nERROR: File not found: {e.filename}\")\n",
    "    print(\"Please ensure your files are in the same directory as this script/notebook.\")\n",
    "    exit()\n",
    "\n",
    "# Separate features (X) and labels (Y). Features are all columns except the last two (Patient ID and Class ID).\n",
    "X_train = train_data.iloc[:, :-2]\n",
    "Y_train = train_data.iloc[:, -1].astype(int) # Class ID\n",
    "\n",
    "X_test = test_data.iloc[:, :-2]\n",
    "Y_test = test_data.iloc[:, -1].astype(int)   # Class ID\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Training and Prediction ---\n",
    "\n",
    "print(\"\\n[STEP 1] Initializing and training the SVC model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# SVC with a linear kernel for multi-class classification\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train, Y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training complete in {training_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "print(\"[STEP 2] Making predictions on the test data...\")\n",
    "start_time = time.time()\n",
    "Y_pred = svm_model.predict(X_test).astype(int)\n",
    "prediction_time = time.time() - start_time\n",
    "print(f\"Prediction complete in {prediction_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "# --- Evaluation ---\n",
    "\n",
    "print(\"\\n[STEP 3] Evaluating the model performance...\")\n",
    "evaluate_model(Y_test, Y_pred, CLASS_IDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06a993",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f66e561c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa0619c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f31c4e6f",
   "metadata": {},
   "source": [
    "6. Use at least one clustering technique to visualize the data and understand better their \n",
    "structure and how well classes are separated. Perform classification based on the cluster \n",
    "features and discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "sns.set()\n",
    "print(\"Imports OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217fdc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading all_data.csv ...\")\n",
    "data = np.genfromtxt(\"./all_data.csv\", delimiter=\",\")\n",
    "\n",
    "# ECG features (277 columns)\n",
    "X = data[:, :-2]\n",
    "\n",
    "# arrhythmia labels (N, L, R, V, A, F, f, /)\n",
    "y = data[:, -2].astype(int)\n",
    "\n",
    "# patient ID (not needed for clustering)\n",
    "patient_id = data[:, -1].astype(int)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac324a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap='tab10', s=2)\n",
    "plt.title(\"PCA Visualization (Colored by True Labels)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=8, random_state=42)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=clusters, cmap='tab10', s=3)\n",
    "plt.title(\"PCA + KMeans Clustering (Colored by Cluster Labels)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d951997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ids = clusters.reshape(-1, 1)\n",
    "distances = kmeans.transform(X)\n",
    "cluster_features = np.hstack([cluster_ids, distances])\n",
    "\n",
    "print(\"Cluster feature shape:\", cluster_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25ce204",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=2000)\n",
    "clf.fit(cluster_features, y)\n",
    "y_pred = clf.predict(cluster_features)\n",
    "\n",
    "print(\"\\n===== Cluster-Based Classification Metrics =====\")\n",
    "print(\"Accuracy:\", accuracy_score(y, y_pred))\n",
    "print(\"Precision:\", precision_score(y, y_pred, average='macro'))\n",
    "print(\"Recall:\", recall_score(y, y_pred, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y, y_pred, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f7386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Cluster-Based Classification)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd02a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dj4e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
